{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "78c19fa0-2f05-4339-8a2b-cdc09c68b975",
      "metadata": {},
      "source": [
        "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3a560a74",
      "metadata": {
        "cell_id": "00000-f53914fc-294a-4bd9-ab99-c7f9f7e072d9",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Proyecto: Innovación Tecnológica en Bodoque Bank\n",
        "\n",
        "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesor: Pablo Badilla, Ignacio Meza De La Jara\n",
        "- Auxiliar: Sebastián Tinoco\n",
        "- Ayudante: Diego Cortez M., Felipe Arias T.\n",
        "\n",
        "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "59ff9d5b",
      "metadata": {
        "cell_id": "00001-7e639802-19f7-4bbe-9a97-e312f613a693",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "----\n",
        "\n",
        "## Reglas\n",
        "\n",
        "- Fecha de entrega: 01/06/2021\n",
        "- **Grupos de 2 personas.**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Estrictamente prohibida la copia. \n",
        "- Pueden usar cualquier material del curso que estimen conveniente."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b020ce37",
      "metadata": {
        "cell_id": "00002-bf13ea5a-d8bf-4cee-879e-ba1c7035e657",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "# Motivación\n",
        "\n",
        "![](https://www.geekmi.news/__export/1621969098810/sites/debate/img/2021/05/25/juan-carlos-bodoque-1.jpg_554688468.jpg)\n",
        "\n",
        "Juan Carlos Bodoque, el famoso periodista y empresario, decidió diversificar su portafolio de negocios y crear su propio banco. Después de varios años de investigar y analizar el mercado financiero, finalmente logró fundar su entidad bancaria con el objetivo de ofrecer a sus clientes una experiencia personalizada y confiable en sus transacciones financieras. \n",
        "\n",
        "Sin embargo, con las nuevas tecnologías, aparecen nuevos desafíos para la joven entidad bancaria. Por ello, Bodoque decide invertir en un equipo de expertos en tecnología y finanzas, para que Bodoque Bank implemente las últimas innovaciones en seguridad y servicio al cliente para garantizar la satisfacción y fidelización de sus clientes.\n",
        "\n",
        "El primer objetivo de la entidad bancaria será la detección de potenciales clientes fraudulentos, para ello Bodoque Bank les hace entrega de un extenso dataset en el que se registran las actividades que han realizado sus clientes durante los últimos meses. Uno de los puntos que resaltan al pasar el conjunto de datos es que el nombre de los usuarios está protegido y que consideren cada una de las filas como una muestra independiente de la otra.\n",
        "\n",
        "Tras la solicitud, uno de los mayores accionistas del banco llamado Mario Hugo, les sugiere que al momento de realizar el proyecto tomen las siguientes consideraciones:\n",
        "\n",
        "- Realicen un análisis exhaustivo de cada variable, ya que los directores tienden a ser quisquillosos con puntos o definiciones sin algún fundamento claro.\n",
        "- Deben ser muy claros de cómo encontrar los outliers y si es posible expliquen cómo están determinando que un cliente es fraudulento.\n",
        "- Es muy probable que existan clientes con diferentes comportamientos bancarios, por lo que sería muy buena decisión de separar a los clientes en grupos etarios.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1223ca1b",
      "metadata": {
        "cell_id": "00003-a5045731-0c9e-47a1-ae7b-243d8603e70b",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "### Definición Formal del Problema\n",
        "\n",
        "El dataset con el que se trabajará en este primer proyecto será [_The Bank Account Fraud (BAF)_](https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022). Este consiste en un dataset para evaluar métodos de detección de fraudes bancarios, el cuál según sus autores es:\n",
        "\n",
        "- Realista: Se basa en un actualizado dataset de casos reales de detección de fraude bancario.\n",
        "- Sesgado: Poseen distintos tipos de bias.\n",
        "- Desbalanceado: Clase positiva extremadamente pequeña.\n",
        "- Dinámico: Presenta datos temporales y cambios de distribución.\n",
        "- Preserva la privacidad: Protege a los potenciales clientes a través de la aplicación de privacidad diferencial.\n",
        "\n",
        "El proyecto tiene por objetivo evaluar los conocimientos adquiridos en la primera mitad del curso, consistente en manejo de datos tabulares (I/O, manipulación, agregaciones, merge y visualizaciones) más la primera parte de modelos consistente en preprocesamiento de datos y detección de anomalías. Por ende, el proyecto consiste en dos tareas principales:\n",
        "\n",
        "1. Generar un Análisis exploratorio de Datos (EDA) que describa completamente el dataset.\n",
        "2. Buscar anomalías de forma automatizada.\n",
        "\n",
        "**Importante:** Esta permitido el uso de librerías externas a las vistas en clases para profundizar aún mas en los análisis. Sin embargo, al momento de utilizar cualqueir metodo deberán explicar que hace y el porque de su aplicación.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f4771943",
      "metadata": {
        "cell_id": "00003-3cb3d532-0223-4021-869f-4b03ea06b875",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "---\n",
        "\n",
        "## Secciones requeridas en la entrega\n",
        "\n",
        "La siguiente lista detalla las secciones que debe contener su notebook para resolver el proyecto. Es importante que al momento de desarrollar cada una de las secciones, estas sean escritas en un formato tipo **informe**, donde describan detalladamente cada uno de los puntos realizados.\n",
        "\n",
        "### 1. Introducción [0.5 Puntos]\n",
        "\n",
        "Elaborar una breve introducción con todo lo necesario para entender qué realizarán durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos básicos tanto del dataset como del análisis a realizar sobre los datos.\n",
        "\n",
        "Por lo anterior, en esta sección ustedes deberán ser capaces de:\n",
        "\n",
        "- Describir la tarea asociada al dataset.\n",
        "- Describir brevemente los datos de entrada que les provee el problema.\n",
        "- Plantear hipótesis de cómo podrían abordar el problema.\n",
        "\n",
        "### 2. Lectura y Manejo de Datos [1 Punto]\n",
        "\n",
        "Este punto tiene por objetivo evaluar la capacidad de leer y manejar distintas fuentes de datos y unirlas.\n",
        "Para esto, se les provee de 3 datasets distintos: \n",
        "\n",
        "- `df_1.parquet` que contiene una cierta cantidad de ejemplos.\n",
        "- `df_2.parquet` que contiene el resto de los ejemplos.\n",
        "- `df_email_phone.parquet` que contiene características exclusivas relacionadas a los emails y teléfonos de los ejemplos.\n",
        "\n",
        "Tanto `df_1` como `df_2` son dataframes que contienen conjuntos (no necesariamente disjuntos) de ejemplos obtenidos a partir de distintas fuentes (la primera más confiable que la segunda)  mientras que `df_email_phone` contiene nuevas características que deben ser fusionadas con el conjunto de ejemplos.\n",
        "Dicho esto, para este punto se le solicita:\n",
        "\n",
        "1. Cargar los datasets usando pandas.\n",
        "2. Explorar superficialmente los datsets por medio de `.head(5)` e `.info()` y reportar número de filas y columnas de cada dataset.\n",
        "3. Unir los datasets con los conjuntos de ejemplos en un único dataset.\n",
        "4. Combinar el datasets con ejemplos con las nuevas características usando la estrategia `outer`.\n",
        "5. Explorar nulos y duplicados de datos (i.e., generar conteos de cada uno y ver algunos de sus ejemplos), explicar su origen y entregar una solución sencilla a estos casos.\n",
        "6. Limpiar datos y reportar modificaciones a los datasets originales (cuántos ejemplos se eliminaron, cuántos ejemplos quedaron finalmente, etc...).\n",
        "7. Realizar una segmentación etaria de los datos creando una columna llamada `segmentacion_etaria` para clasificar a los diferentes usuarios. Para la segmentación utilice los siguientes rangos:\n",
        "   - Joven (Menor a 18 años)\n",
        "   - Adulto-Joven (18 - 26 años)\n",
        "   - Adulto (27- 59 años)\n",
        "   - Persona Mayor (60 años o mas).\n",
        "\n",
        "\n",
        "### 3. Análisis Exploratorio de Datos [1 Punto]\n",
        "\n",
        "Esta sección se busca evaluar la capacidad de generar reportes y conclusiones a partir de los patrones observados en estos.\n",
        "\n",
        "- Utilizando `pandas-profiling` (nombre de la librería: `ydata-profiling`), explore:\n",
        "\n",
        "    * Analizar cantidad de datos nulos, tipos de datos, duplicados, distribuciones de las variables a través de histogramas.\n",
        "    * Generar visualizaciones de las interacciones (como por ejemplo, una scatter matrix) en las distintas variables.\n",
        "    * Ver las correlaciones entre las distintas variables y los valores faltantes de cada una de estas. \n",
        "    * Reportar los patrones interesantes observados. Ejemplos:\n",
        "        - Se observó que las variables numéricas A, B y C siguen una distribución normal / exponencial / logarítmica / no presenta distribución conocida debido a que (usar información sobre el descripción de cada variable)....\n",
        "        - La mayoría de los ejemplos de las variables I, J y K tienen valor 0.\n",
        "        - La variable M presenta gran cantidad de outliers.\n",
        "        - Se observó que las variables categóricas E y F están bien balanceadas / presentan un gran desbalance.\n",
        "        - Del gráfico de correlaciones se puede inferir que X, Y y Z tienen una correlación alta/baja entre ellas.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Preparación de Datos [1 Punto]\n",
        "\n",
        "*Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo.*\n",
        "\n",
        "Dentro de los aspectos minimos que se les solicitarán en este desarrollo estan:\n",
        "\n",
        "- Definir preprocesadores para datos categóricos y ordinales.\n",
        "- Setear las transformaciones en un `ColumnTransformer`.\n",
        "- Transformar todo el datset.\n",
        "\n",
        "**Notas**: \n",
        "\n",
        "- Las variables `id` y `fraud_bool` no deben ser transformadas, pero si pasadas a la siguiente etapa. (Estudiar la transformación `passthrough`).\n",
        "- La salida de la transformación debe ser un pandas dataframe. Para esto, investiguen acerca de la nueva [`set_output` API](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html). Si se levanta un error por matriz sparse, desactiven su uso en la transformación correspondiente.\n",
        "\n",
        "### 5. Visualización en baja dimensionalidad [1 Punto]\n",
        "\n",
        "Programar un script para proyectar los datos en baja dimensionalidad usando `PCA` o `UMAP`.\n",
        "\n",
        "**Notas**: \n",
        "\n",
        "- Para realizar las proyecciones no consideren la variable `fraud_bool`, ya que esto provocaría un _data leakage_ (i.e., usar información del futuro para analizar el mismo fenómeno que quieren descubrir).\n",
        "- Utilice un muestreo significativo del dataframe (10k a 25k ejemplos) para generar las proyecciones. De lo contrario, probablemente el proyector no sea capaz de asignar todos los recursos necesarios y lance alguna excepción.\n",
        "\n",
        "Luego, por cada segmento etario generado en la sección 2, implemente un gráfico de dispersión (idealmente en plotly express) la proyección en 2D generada y coloree cada punto según la etiqueta `fraud_bool`.\n",
        "\n",
        "Reportar si existen patrones y/o relaciones interesantes con respecto a variables de interés.\n",
        "\n",
        "\n",
        "### 6. Explorar Anomalías [1.5 Puntos]\n",
        "\n",
        "Proponer una técnica para detectar clientes fraudulentos en base a detección de anomalías. \n",
        "Al igual que el punto anterior, la técnica que proponga debe ser aplicada por separado en los diferentes segmentos etarios descritos en el punto 2 (es decir, debe entrenar un detector de anomalías para cada uno de estos grupos). \n",
        "\n",
        "Luego, al igual que el punto anterior, genere un gráfico de dispersión con las proyecciones 2D en donde el coloreado sea ahora las etiquetas predichas por el detector de anomalías.\n",
        "\n",
        "Por último, usando queries de numpy/pandas, calcule dos ratios (P y R):\n",
        "\n",
        "- P: Cantidad de ejemplos predichos correctamente como fraude / cantidad total de datos predichos como fraude.\n",
        "- R: Cantidad de ejemplos predichos correctamente como fraude / cantidad total de ejemplos que eran realmente fraude.\n",
        "\n",
        "\n",
        "De los resultados obtenidos responda las siguientes preguntas:\n",
        "\n",
        "- ¿Qué significan los ratios y sus valores?\n",
        "- ¿Qué tan correctas fueron las predicciones realizadas por su modelo según los ratios calculados?\n",
        "- ¿Son coherentes los resultados obtenidos?.\n",
        "- ¿Cómo se comportan los casos de fraudes para los diferentes rangos etarios?\n",
        "\n",
        "Justifique cada una de sus respuestas."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9b045cec",
      "metadata": {
        "cell_id": "00004-2c33dfb6-7731-4252-a108-cec08f45ddf9",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "---\n",
        "\n",
        "## Esquema de la Tarea\n",
        "\n",
        "\n",
        "Pueden usar el siguiente esquema para organizar la tarea (y borrar todo lo anterior).\n",
        "Obviamente **no deben limitarse a lo que está escrito en esta**: puede incrementar en caso de más técnicas y obviar algunas partes en caso que alguna y otro punto no aplique a su problema.\n",
        "\n",
        "Pueden borrar las instrucciones anteriores y quedarse solo con lo que viene a continuación.\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d8c982ca",
      "metadata": {
        "cell_id": "00005-7921fb53-59e9-49a1-a5a5-dec92da88299",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "\n",
        "\n",
        "# Proyecto\n",
        "\n",
        "### Equipo:\n",
        "\n",
        "- Sebastián Versluys\n",
        "- Josué Guillen\n",
        "\n",
        "\n",
        "### Link de repositorio de GitHub: `https://github.com/Nietsabas/MDS7202`\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "046cceb7",
      "metadata": {
        "cell_id": "00006-200f2a79-7517-4226-8a5b-438553fa1b13",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "\n",
        "## 1. Introducción\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ec3873",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "720c912c-e759-4258-8085-5b3a0f4a3027",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## 2. Lectura y Manejo de Datos\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3c72c649-c228-43dd-adfd-16e3896203f3",
      "metadata": {},
      "source": [
        "### 2.1 Cargar datos en dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8075dfb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#Instalando librerias necesarias para leer archivos en formato .parquet\n",
        "!pip install pyarrow\n",
        "!pip install fastparquet\n",
        "\n",
        "# Cargar df_1.parquet\n",
        "df_1 = pd.read_parquet('df_1.parquet')\n",
        "\n",
        "# Cargar df_2.parquet\n",
        "df_2 = pd.read_parquet('df_2.parquet')\n",
        "\n",
        "# Cargar df_email_phone.parquet\n",
        "df_email_phone = pd.read_parquet('df_email_phone.parquet')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e6af3714-cc14-495a-8757-115e7cde7d31",
      "metadata": {},
      "source": [
        "### 2.2 Exploración inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8469fe2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploración df_1.parquet\n",
        "print(\"===================================== df_1.parquet =========================================\")\n",
        "print(\"Número de filas y columnas:\", df_1.shape)\n",
        "print(\"Información del dataset:\")\n",
        "df_1.info()\n",
        "print()\n",
        "print(\"Primeras 5 filas:\")\n",
        "df_1.head(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "149d4f33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploración df_2.parquet\n",
        "print(\"===================================== df_2.parquet =========================================\")\n",
        "print(\"Número de filas y columnas:\", df_2.shape)\n",
        "print(\"Información del dataset:\")\n",
        "df_2.info()\n",
        "print()\n",
        "print(\"Primeras 5 filas:\")\n",
        "df_2.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploración df_email_phone\n",
        "print(\"===================================== df_email_phone.parquet =========================================\")\n",
        "print(\"Número de filas y columnas:\", df_email_phone.shape)\n",
        "print(\"Información del dataset:\")\n",
        "df_email_phone.info()\n",
        "print(\"Primeras 5 filas:\")\n",
        "df_email_phone.head(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "882a9ff0-9399-43c7-9b7d-108e36ddef24",
      "metadata": {},
      "source": [
        "### 2.3 Unir dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09bea965",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat([df_1, df_2])\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f2e5bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_nulos = df.isnull().sum()\n",
        "df_nulos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3359abbb-aa14-4c03-b108-cc7c0baa4031",
      "metadata": {},
      "source": [
        "### 2.4 Unir nuevas variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd0d917",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.merge(df, df_email_phone, on='id', how='outer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8538eed",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034e13f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "813bab66-81d8-421f-921d-109a7e474790",
      "metadata": {},
      "source": [
        "### 2.5 Verificar nulos y duplicados, generar explicaciones sobre sus fuentes y proponer soluciones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c5ce09",
      "metadata": {},
      "outputs": [],
      "source": [
        "valores_nulos = df.isnull().sum()\n",
        "print(\"Conteo de valores nulos por columna:\")\n",
        "valores_nulos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014afe73",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_filas_duplicadas = df.duplicated().sum()\n",
        "print(\"\\nConteo de filas duplicados:\", num_filas_duplicadas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ccad09",
      "metadata": {},
      "outputs": [],
      "source": [
        "filas_con_NA = df[df.isnull().any(axis=1)].head()\n",
        "print(\"\\nEjemplos de filas con valores nulos:\")\n",
        "filas_con_NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a4b9a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "filas_duplicadas = df[df.duplicated(keep=False)].head()\n",
        "print(\"\\nEjemplos de filas con valores duplicados:\")\n",
        "filas_duplicadas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "37e73203",
      "metadata": {},
      "source": [
        "Veamos el origen de estos duplicados y NA values, para ello primero analicemos si los valores nulos o filas duplicadas provienen de los datasets originales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc8787f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "nulos_df_1 = df_1.isnull().sum()\n",
        "nulos_df_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887c3221",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_filas_duplicadas_df_1 = df_1.duplicated().sum()\n",
        "print(\"\\nConteo de filas duplicados:\", num_filas_duplicadas_df_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb2eba6",
      "metadata": {},
      "outputs": [],
      "source": [
        "nulos_df_2 = df_2.isnull().sum()\n",
        "nulos_df_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b727f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_filas_duplicadas_df_2 = df_2.duplicated().sum()\n",
        "print(\"\\nConteo de filas duplicados:\", num_filas_duplicadas_df_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ca3ea4",
      "metadata": {},
      "outputs": [],
      "source": [
        "nulos_df_email_phone = df_email_phone.isnull().sum()\n",
        "nulos_df_email_phone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff41d0eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_filas_duplicadas_df_email_phone = df_email_phone.duplicated().sum()\n",
        "print(\"\\nConteo de filas duplicados:\", num_filas_duplicadas_df_email_phone)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "388ea1b5",
      "metadata": {},
      "source": [
        "Después de este análisis, podemos darnos cuenta que:\n",
        "\n",
        "- Valores nulos (NA): Se observa que varias columnas del df resultante tienen una cantidad significativa de valores nulos. Por ejemplo, las columnas \"fraud_bool_x\", \"income_x\", \"prev_address_months_count_x\", \"current_address_months_count_x\" y otras presentan un alto número de valores nulos. Podemos ver que los nulos provienen exclusivamente del proceso de merge que se hizo entre la concatenación de df_1 y df_2 con df_email_phone, ya que si se unen datasets con diferentes formatos de registro o claves primarias inconsistentes, pueden surgir valores nulos.\n",
        "\n",
        "- Filas duplicadas: Se encontraron ejemplos de filas duplicadas en el dataset df resultante del merge que aproximadamente son 202334 filas duplicadas. Por ejemplo, la fila con el ID 729517 o 149585 se repite varias veces, lo que indica la presencia de duplicados para este ID. Esto se debe a que df_2 ya contenia duplicados aproximadamente 163292 y lo otra parte se generó durante el proceso de merge  de los datasets  df_1 y df_2 con df_email_phone."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0e04ff63",
      "metadata": {},
      "source": [
        "Para abordar estos problemas, se puede hacer lo siguiente:\n",
        "\n",
        "- Eliminación de filas con valores nulos:\n",
        "\n",
        "Se pueden eliminar las filas que contienen valores nulos en alguna de las columnas utilizando el método dropna(), esto reduciría el dataset y evitaría posibles problemas en el análisis posterior al no haber datos incompletos y brindar consistencia al dataset, sin embargo esto también puede reducir significativamente el tamaño del dataset, lo que implica una pérdida de información potencial y falta de representatividad y generalización de los resultados posteriores al hacer algún análisis.\n",
        "\n",
        "- Imputación de valores nulos:\n",
        "\n",
        "Esta estrategía consiste en reemplazar los valores nulos con valores estimados o inferidos a partir de la información disponible en el dataset, esto permite aprovechar la información de las demás columnas y conservar el tamaño original del dataset, además de evitar la pérdida de datos y mantiene la representatividad de las muestras. La gran desventaja de esta imputación es que es posible intoducir sesgo en la misma, por lo que la elección de una técnica de imputación adecuada podría ser crucial.\n",
        "\n",
        "- Eliminación de filas duplicadas:\n",
        "\n",
        "Si las filas duplicadas no aportan información adicional o son erróneas, se pueden eliminar utilizando el método drop_duplicates(). No cabe dudas que eliminar las filas duplicadas en el dataset resultante es necesario ya que tener filas duplicadas introduce sesgos en el análisis, por lo que tratar esto asegura la unicidad de los datos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "edf92eba",
      "metadata": {},
      "source": [
        "En el siguiente punto se analizará la estrategia a utilizar con la justificación respectiva:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "31b39557-4e12-4961-9a96-60d7eb945080",
      "metadata": {},
      "source": [
        "### 2.6 Limpiar"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "48bee72a",
      "metadata": {},
      "source": [
        "Veamos primero el porcentaje de duplicados del dataset df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65507c50",
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicados = df.duplicated()\n",
        "porcentaje_duplicados = (duplicados.sum() / len(df)) * 100\n",
        "print(f\"Porcentaje de duplicados: {porcentaje_duplicados:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "155a2e81",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3d75d621",
      "metadata": {},
      "source": [
        "Notemos que el porcentaje de duplicados es apenas el 16.89% de los datos, teniendo aproximadamente df shape  de (1198151, 35), por lo que podemos eliminar los duplicados, en efecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd1eefd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()\n",
        "df.shape\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9b2b207a",
      "metadata": {},
      "source": [
        "Notemos que luego de la eliminación aún seguimos conservando gran cantidad de los datos pasamos de 1198151 con un 16% de valores duplicados a 995817 sin muestras duplicadas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4751a797",
      "metadata": {},
      "source": [
        "Veamos ahora que haremos con los nulos, primero veamos el porcentaje de los valores nulos en cada columna:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09688df0",
      "metadata": {},
      "outputs": [],
      "source": [
        "porcentaje_nulos = (df.isnull().sum() / len(df)) * 100\n",
        "print(\"Porcentaje de nulos por columna:\")\n",
        "porcentaje_nulos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1156cc31",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sin_nulos = df.dropna()\n",
        "\n",
        "porcentaje_eliminado = ((len(df) - len(df_sin_nulos)) / len(df)) * 100\n",
        "\n",
        "print(f\"Porcentaje de filas eliminadas: {porcentaje_eliminado:.2f}%\")\n",
        "print(f\"Shape del df sin nulos:\")\n",
        "df_sin_nulos.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "52aee623",
      "metadata": {},
      "source": [
        "Notemos que apenas los nulos para cada columna son el 6.13% y al eliminarlos el dataset se reduce en un 12,26% pasando de 995817 a  873771 por lo que podemos eliminarlos, ya que no perderemos demasiada información, en efecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e520e3a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_sin_nulos\n",
        "df.head(5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b211f607",
      "metadata": {},
      "source": [
        "### 2.7 Segmentación"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3ea04052",
      "metadata": {},
      "source": [
        "Hagamos ahora una segmentacion etaria en base a lo que nos dice el enunciado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a28d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "''' \n",
        "Usaremos la funcion cut de pandas, pues esta se utiliza\n",
        "para discretizar o dividir una variable continua en intervalos o categorías específicas\n",
        "\n",
        "Para ello definamos los rangos:\n",
        "\n",
        "'''\n",
        "\n",
        "rangos = [0, 18, 27, 60, float('inf')]\n",
        "etiquetas = [\"Joven\", \"Adulto-Joven\", \"Adulto\", \"Persona Mayor\"]\n",
        "\n",
        "df[\"segmentacion_etaria\"] = pd.cut(df[\"customer_age\"], bins=rangos, labels=etiquetas, right=False)\n",
        "df.head(5)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "576f4495",
      "metadata": {
        "cell_id": "00007-d1ce17da-f6e0-4c12-bbe8-a40a46e7371b",
        "deepnote_cell_type": "markdown",
        "tags": []
      },
      "source": [
        "## 3. Análisis Exploratorio de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943cbe73",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ydata_profiling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27056a96",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport(df, title=\"EDA\")\n",
        "profile.to_file(\"eda.html\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9faaf197",
      "metadata": {},
      "source": [
        "### 3.1 Análisis del EDA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "48416d3f",
      "metadata": {},
      "source": [
        "- Se puede notar primeramente que no existen valores nulos según la matriz de Missing values que se generó, esto es razonable ya que hicimos un tratamiento respecto a esto, sin embargo como veremos en este análisis, hay alguna variables que usan negativos para guardar missing values.\n",
        "- Respecto a filas duplicadas, estas no existen, ya que hicimos tratamiento de esto.\n",
        "- Se tienen 873771 y 36 variables con un espacio en memoria de 240.8 MiB\n",
        "\n",
        "Respecto a las distribuciones de las variables se puede notar lo siguiente:\n",
        "\n",
        "- Los IDs van de 0 a 999999 y no hay IDs repetidos ya que hay 873771 distinct values, ademas que la distribucion es uniforme en los reales\n",
        "\n",
        "- La variable fraud_bool esta claramente desbalanceada hacia aplicaciones o registros no fraudulentos 98.9%  vs 1.1% fraudulentos. \n",
        "\n",
        "- La variable income es una variable que agrupa en deciles segun el nivel de sueldo de las personas, va desde el decil 0.1 a 0.9, ademas  se observa que el primer cuartil (Q1) es 0.3, la mediana es 0.6 y el tercer cuartil (Q3) es 0.8 Además se puede notar que el valor más frecuente es 0.9, seguido de 0.8 y 0.1, mientras que los valores menos frecuentes son 0.5, 0.3 y 0.2. \n",
        "Se puede notar que la variable income presenta una distribución desigual, se asemeja  a una distribución exponencial con una mayor concentración de valores en los deciles superiores (0.8 y 0.9), hay un claro desbalance de los datos, ya que se tienen personas con ingresos muy bajos que son el 14.5% del decil 0.1 y demasiadas personas con ingresos altos, ya que  39.2 % son del decil 8 y 9, esto indica que hay muchos solicitantes que tienen ingresos altos en comparación con los de ingresos medianamente bajos o intermedios.\n",
        "\n",
        "- Para la variable prev_address_months_count se observa que la mayoría de los registros (76.2%) tienen un valor de -1, lo que indica que no se dispone de información sobre la dirección anterior. Esto sugiere un desequilibrio significativo en los datos, donde la gran mayoría de los solicitantes no tienen información registrada sobre su residencia anterior.\n",
        "Se observa que el el rango es de 400 y el tercer cuartil (Q3) es -1. Además la asimetría (skewness) es bastante alta, con un valor de 4.321071, lo cual indica una cola larga en la distribución. Esta alta asimetría en conjunto con esta distribución hacia mayores valores con registros -1, puede ser un indicio de la presencia de outliers en los extremos de la distribución. En cuanto al tipo de distribución, esta se asemeja a una distribución logarítmica o 1/x con un alto desequilibrio de los datos.\n",
        "\n",
        "- La variable current_address_months_count muestra una distribución asimétrica positiva, con una mayor concentración de valores en el extremo inferior. Con un rango intercuartil de 127 y una mediana de 64, la distribucion se asemeja a un logaritmo y es una distribucion decreciente, esto muestra que la mayoria de clientes lleva poco tiempo en su dirección actual, aunque segun las estadisticas descriptivas existen clientes que llevan mucho tiempo en su dirección actual que son basicamente outliers en el extremo superior de la distribución.\n",
        "\n",
        "- La variable customer_age muestra una distribución relativamente simétrica parecida a una normal, con una concentración notable de valores alrededor de las décadas de 30, 40 y 50. El primer cuartil (Q1) se encuentra en 30 años, la mediana en 50 años y el tercer cuartil (Q3) en 50 años también. Esto indica que una gran proporción de solicitantes tiene edades comprendidas entre los 30 y 50 años. Además, se observa que la desviación estándar es de 13.76, lo que indica una dispersión moderada de las edades. La variable muestra una ligera asimetría negativa -0.20, lo que sugiere que hay una ligera tendencia hacia edades relativamente jóvenes menores a 50 años En cuanto a los valores atípicos, se encuentran pocos casos en las edades de 80 y 90 años, que representan menos del 1% de los clientes. En general, la distribución de la variable \"customer_age\" se caracteriza por una concentración en edades de adulto y adulto joven y una disminución gradual a medida que aumenta la edad.\n",
        "\n",
        "- La variable days_since_request muestra una distribución altamente asimétrica positiva con un valor de 9.976, con una concentración notable de valores cercanos a cero. El primer cuartil (Q1) se encuentra en 0.007 días, la mediana en 0.016 días y el tercer cuartil (Q3) en 0.027 días, además la desviación estándar de 4.997 y el coeficiente de variación de 5.536 indican una dispersión significativa. Esto indica que la mayoría de las solicitudes se realizaron recientemente en días o incluso horas recientes. En cuanto a los posibles valores atípicos, hay unos cuantos valores que representan que representan menos del 0.1% de las solicitudes, esto indica que hay algunas solicitudes que se realizaron hace más de un 1 día y hay otras pocas que se realizan hace menos de 0.036 días también representando todas estas menos 1% de los datos.\n",
        "\n",
        "- La variable intended_balcon_amount muestra una distribución asimétrica positiva con un valor de 2.585, con una concentración de valores en el extremo inferior. El primer cuartil (Q1) se encuentra en -1.180, la mediana en -0.835 y el tercer cuartil (Q3) en -0.066. Esto indica que la mayoría de los solicitantes tienen la intención de transferir montos relativamente pequeños. En cuanto a los posibles valores atípicos, hay personas con intención de transferir montos grandes o medianamente grandes en torno a 112 (menos del 1%) pero son muy pocas.\n",
        "\n",
        "- Para la variable payment_type se puede notar que el valor \"AB\" aparece con mayor frecuencia, con un total de 348,705 casos, lo que representa el 39.9% de las observaciones. Le sigue el valor \"AA\" con 217,944 casos (24.9%) y luego el valor \"AC\" con 215,904 casos (24.7%). El valor \"AD\" tiene una frecuencia de 91,011 casos, lo que equivale al 10.4% de las observaciones. Finalmente, el valor \"AE\" tiene una frecuencia muy baja, con solo 207 casos, lo que representa menos del 0.1% de las observaciones. Esto implica un claro desbalance de clases  con una predominancia o preferencia por los tipos de pago AB, AA y AC, y con poca preferencia por los  AD y AE.\n",
        "\n",
        "- Para zip_count_4w podemos observar que el rango de valores es bastante amplio, el valor mínimo registrado es 1, el percentil 5 es 492, el primer cuartil (Q1) es 886, la mediana es 1208, el tercer cuartil (Q3) es 1844 y el percentil 95 es 3544. El promedio de solicitudes por código postal es de aproximadamente 1518, lo que sugiere que, en general, hay un número considerable de solicitudes en cada área. Sin embargo, la dispersión de los datos es alta, como se refleja en la desviación estándar de 965. Esto significa que la cantidad de solicitudes puede variar significativamente de un código postal a otro. Hay algunos códigos postales con una mayor concentración de solicitudes, como los valores 1014 o 924. Estos códigos postales pueden ser áreas de mayor interés o demanda potencial a prestar atención. La distribución es un tanto variable pero es parecida a una normal centrada o con una mayor cantidad de datos en torno a la mediana y el quartil 3.\n",
        "\n",
        "- La variable velocity_6h muestra la variabilidad en la velocidad de las solicitudes de crédito en un período de 6 horas. Los datos muestran que hay momentos de mayor velocidad de solicitudes y momentos de menor velocidad es mas el valor mínimo es -174.11 o disminución de solicitudes de crédito, se tiene además una mediana de 5190 con un rango intercuartil de 4033.95, la distribución es una normal clara con una desviación muy alta de 2940.4648 existen outliers debido a valores muy altos de 12500 para arriba, pero en general no hay presencia de valores extremadamente alejados de la media.\n",
        "\n",
        "- Lo mismo sucede con velocity_24h y velocity_4w ambas tienen cierta simetría normal con promedios muy parecidos de 4660.4553 y 4734.2333 respectivamente, claramente hay outliers que son los momentos cuando hay demasiadas solicitudes o muy pero muy pocas.\n",
        "\n",
        "- La variable bank_branch_count_8w presenta una distribución demasiado desigual concentrada en 0 y 31 con una mayor frecuencia de 0 y 1 sucursales, esto podría indicar áreas o momentos en los que no hay sucursales bancarias disponibles. La mediana esta en 10 sucursales, la desviación es muy alta de 473.42, con todo esto es claro que existen demasiados outliers con valores muy altos, esto podría deberse a que pocas veces hay muchas sucursales operativas en un periodo de 8 semanas, esta variable podría estar evidenciando que existe poca disponibilidad de sucursales y una alta variabilidad de las mismas, por lo que se debería prestar atención ya que podríamos estar desaprovechando la demanda de clientes o teniendo un problema logístico grave. \n",
        "\n",
        "- Con respecto a la variable employment_status claramente hay un desbalance en torno a la clase CA con un recuento de 598,651 personas o un 68.5% de los datos.\n",
        "\n",
        "- Para la variable credit_risk_score se puede observar una normal muy marcada en torno a una media  de 139.2571 el IQR es de 98 y la desviacion estándar es relativamente alta de 71.432183, esto indica que existe cierta cantidad de outliers tanto en personas con un credit risk score muy bajo y personas con uno muy alto superior al 95 percentil que es 267. Existe igualmente score negativo lo cual indica que hay ciertas personas con un score muy pero muy malo.\n",
        "\n",
        "- Respecto a la variable housing_status hay una clara tendencia a personas con casas con status BA, BB y BC siendo estas dos últimas la de mayor frecuencia 30.1% y 33.5% respectivamente.\n",
        "\n",
        "- Se puede observar para\n",
        "bank_months_count, que la mayoría de las cuentas bancarias tienen una duración relativamente corta, con una mediana de 6 meses y el 75% de las cuentas duran 25 meses o menos, además la distribución es bastante desigual pero en general con una gran cantidad de datos 95 percentil (menor a 30 meses). Sin embargo, es importante tener en cuenta que existen valores de meses negativos en los percentiles más bajos, ya que podrían indicar problemas al registrar los meses de duración de las cuentas o que el mes -1 por ejemplo signifique algo interno, como cuenta que aún no se ha abierto, etc.\n",
        "\n",
        "- Según la variable has_other_cards que es un bool, existe un 75% (bool 0) y 25% (bool 1) de clientes que no tienen y que tienen otras tarjetas de débito o crédito respectivamente, esto presenta un claro desbalance y podría ser una oportunidad de mejora para ofrecer tarjetas o cuentas mas atractivas para que mas clientes tengan otras tarjetas.\n",
        "\n",
        "- Según la variable foreign_request en general se han realizado pocas solicitudes relacionadas con el extranjero, con un 97.6% (bool 0) y 2.4% (bool 1).\n",
        "\n",
        "- Según la variable source, la mayoría de solicitudes de aplicación se hicieron vía internet 99.2% seguido de teleapp con un 0.8%, este desbalance si bien es demasiado alto es propio de este tipo de servicios, además de que en general por seguridad en estos tiempos las personas estan prefiriendo canales online.\n",
        "\n",
        "- Según la variable session_length_in_minutes se puede observar un claro logaritmo, concentrandose la duración de la sesion en pocas minutos la media es 7.81 minutos y el percentil 95 es de 22.53 minutos, con un IQR de 6.2 y una desviacion de 8.24 aprox, lo que sugiere que en general las personas duran poco tiempo en la sesión, existen valores atípicos a considerar por ejemplo minutos con valor -1 en el 0.2% de las muestras esto podría ser un error o algún estándar de inputacion para esta variable por parte del banco, también existen según el histograma outliers mayores a 23 minutos, esto podría deberse a personas que dejan abierto la sesion mientras hacen otras cosas, también hay outliers de personas que se demoran segundos en la sesión lo que podría deberse a personas que simplemente se arrepintieron de hacer la solicitud o salieron de improviso.\n",
        "\n",
        "- Con respecto a la variable device_os un porcentaje considerable de los dispositivos utilizan el sistema operativo Linux 33.7%, windows 30.4%, Macintosh 5%, x11 que es comúnmente asociado con sistemas Unix y Linux (0.8%). Notemos que si bien hay un pequeño desbalance de clases en general, los OS de los dispositivos son uniformes, además notemos que Android esta hecho a base de linux y otros podría deberse a sistemas IOS de Iphone por lo que tiene sentido este desbalance.\n",
        "\n",
        "- La variable keep_alive_session indica que la mayoría de las interacciones o transacciones se mantuvo una sesión activa 55.6% un 44.4% de las veces no. En general esta muy balanceado esto sin embargo como banco queremos un mayor numer de transacciones con un alive session ya que asi tendriamos mas probilidades de que el cliente realize una transaccion o solicitud. \n",
        "\n",
        "- Para la variable device_fraud_count no se detectaron aplicaciones fraudulentas basadas en el dispositivo utilizado en el conjunto de datos analizado. Claramente hay un desbalance, sin embargo esto podría ser bueno ya que un porcentaje del 100% de aplicaciones no fraudulentas en relación al dispositivo implica que las medidas de seguridad del banco en general son buenas.\n",
        "\n",
        "- La variable month es simetrica con una normal, no se visualizan posibles outliers, ya que existe una distribución relativamente equilibrada de solicitudes a lo largo de los meses (sesgo es cercano a cero -0.057323507), con una mayor concentración en los meses 2, 3, 4 y 5.\n",
        "\n",
        "- Para la variable x1 se tiene una distribución normal muy marcada que se extiende desde valores negativos (-4.97) hasta valores positivos, con una concentración en el rango intercuartil y una mediana de 0.0067. Los datos muestran una dispersión moderada y una ligera e imperceptible asimetría hacia la derecha (Skewness\t0.065)\n",
        "\n",
        "- Para la variable x2 se tiene una distribución normal muy marcada que se extiende desde valores negativos (-4.84) hasta valores positivos, la distribución de la variable muestra una concentración en el rango intercuartil (Q1 a Q3) alrededor de la mediana. La desviación estándar es 1.0124485, lo que indica una dispersión moderada de los datos alrededor de la media.\n",
        "\n",
        "- Para la variable name_email_similarity revela una distribución de la variable que no se asemeja a una distribución normal. El sesgo (skewness) es ligeramente positivo (0.0704), lo que indica una ligera asimetría hacia la derecha. Además, la curtosis es de de -1.308 sugiere una distribución relativamente plana con colas más ligeras que una distribución normal. Además la variable muestra una concentración de valores alrededor de la mediana, que es 0.4860. Esto indica que la mayoría de las solicitudes tienen una similitud moderada entre el nombre y el correo electrónico del solicitante. En resúmen según se puede observar en general las personas tratan de tener un correo lo más parecido a su nombre, sin embargo por motivos ajenos a este estudio, existen igualmente personas que tienen una baja similitud entre nombre y correo a lo hora de hacer su solicitud.\n",
        "\n",
        "- Según la variable \"date_of_birth_distinct_emails_4w\", se puede observar una distribución que se concentra en un rango de valores de pocos correos distintos. La media de esta variable es de 7.77, y el percentil 95 es de 17. Esto indica que la mayoría de las muestras se encuentran en un rango relativamente estrecho.\n",
        "El rango intercuartílico (IQR) es de 7, lo que indica que la dispersión de los datos dentro del rango intercuartílico es moderada. La desviación estándar es de aproximadamente 4.82, lo que sugiere una variabilidad considerable en los datos.\n",
        "Se observa una frecuencia significativa de valores como 6, 5, 4, y 7, lo que indica una concentración de muestras en esos valores específicos. Además, hay outliers en el extremo superior del rango, con valores mayores a 23, estos outliers representan casos inusuales en los que las personas tienen una cantidad alta de correos electrónicos distintos asociados a su fecha de nacimiento en un período de 4 semanas.\n",
        "\n",
        "- Para la variable \"email_is_free\" existen dos clases el valor 1.0, que indica que el correo electrónico es gratuito y el valor 0.0, que indica que el correo electrónico no es gratuito. Según se puede observar la mayoría de las muestras tienen correos electrónicos gratuitos, mientras que una proporción ligeramente menor tiene correos electrónicos que no son gratuitos.\n",
        "\n",
        "- La variable \"device_distinct_emails_8w\" muestra una predominancia abrumadora del valor 1.0 (96.4%), lo que indica que la mayoría de las personas utilizan un único dispositivo para acceder a los correos electrónicos en un período de 8 semanas. Sin embargo, también existen casos en los que se utilizan dos dispositivos distintos o ninguno. La presencia de valores atípicos como -1.0 (0.1%) indica la necesidad de considerar posibles errores de inputación para estas muestras. Algo que nos llamo la atencion es que hay muestras con valor 0.0 lo que indica que hubieron personas que no accedieron en un periodo de 8 semanas seguidas mediante su dispositivo.\n",
        "\n",
        "- La variable \"phone_home_valid\" muestra una distribución equilibrada entre los valores 0.0 y 1.0, lo que indica que hay una proporción similar de personas con números de teléfono de casa válidos (50.7%) y 49.3% no válidos en las muestras.\n",
        "\n",
        "- La variable \"phone_mobile_valid\" muestra que la mayoría de las personas tienen un número de teléfono móvil válido registrado (85.7%), aunque existe una proporción significativa de personas que no tienen un número válido o no proporcionaron información sobre su número de teléfono móvil (14.3%).\n",
        "\n",
        "- La variable segmentacion_etaria la creamos a partir de customer_age (numeric) y como se puede apreciar en la distribución y tal cual lo dijimos para la variable customer_age (numeric) las solicitudes estan concentradas en personas Adultas y Adultas jóvenes, aunque existen personas jóvenes (menores a 18 años) y personas mayores que representan el 12.2% (Mayor a 60 años)\n",
        "\n",
        "\n",
        "En resumen, existen 5 variables que nos llaman mucho la atención: \n",
        "\n",
        "- \"income\": Esta variable muestra una concentración de valores en los deciles superiores, lo que indica un desbalance en los ingresos de los solicitantes, con una gran cantidad de registros de ingresos altos en comparación con los ingresos bajos o intermedios.\n",
        "\n",
        "- \"prev_address_months_count\" y \"current_address_months_count\": Ambas variables tienen una fuerte concentracion en el valor -1, lo cual sugiere una falta de información sobre las direcciones anteriores y actuales de los solicitantes. Esto podría ser un indicio de datos faltantes que se debería corregir.\n",
        "\n",
        "- \"fraud_bool\": Esta variable muestra un gran desbalance de clases, con el 98.9% de los registros correspondiendo a aplicaciones no fraudulentas y solo el 1.1% siendo fraudulentas. Este desequilibrio puede afectar el rendimiento de los modelos de detección de fraudes y requerir técnicas de muestreo o ajuste.\n",
        "\n",
        "- \"payment_type\": Esta variable presenta un desbalance de clases, con algunos tipos de pago siendo mucho más frecuentes que otros. Esto puede influir en el análisis de patrones de comportamiento de pago.\n",
        "\n",
        "- \"days_since_request\": Esta variable muestra una concentración de valores cercanos a cero, lo que indica que la mayoría de las solicitudes se realizaron recientemente. Esta información puede ser útil para comprender la frecuencia de solicitudes y puede ser relevante para la gestión de solicitudes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4cf49c31",
      "metadata": {},
      "source": [
        "Con respecto a las correlaciones, usando el Heatmap del EDA se puede observar ciertas correlaciones muy interesantes.\n",
        "\n",
        "- Algunas variables  están altamente correlacionadas entre sí, lo cual puede indicar una fuerte relación o dependencia entre ellas. Por ejemplo, \"velocity_6h\", \"velocity_24h\" y \"velocity_4w\" están altamente correlacionadas entre sí, lo que sugiere que están midiendo conceptos similares.\n",
        "\n",
        "- La variable customer_age claramente tiene y esta altamente correlacionada con segmentacion_etaria\n",
        "\n",
        "-  La variable \"income\" tiene una correlación positiva moderada con las variables \"intended_balcon_amount\" (0.108) esto indica que a medida que el ingreso de los clientes aumenta, tienden a solicitar montos más altos para préstamos o créditos y  también entre income y \"credit_risk_score\" (0.186), esto indica que los clientes con ingresos más altos tienden a tener una mejor puntuación de riesgo crediticio.\n",
        "\n",
        "- La variable \"prev_address_months_count\" muestra una correlación negativa fuerte con la variable \"current_address_months_count\" (-0.611), lo que indica que hay una tendencia a que los clientes se muden con menos frecuencia.\n",
        "\n",
        "- La variable \"customer_age\" tiene una correlación positiva moderada con las variables \"days_since_request\" (0.039) y \"session_length_in_minutes\" (0.054), lo que sugiere que los clientes más jóvenes tienden a solicitar servicios con mayor frecuencia y tener sesiones más largas.\n",
        "\n",
        "- La variable \"velocity_6h\" está fuertemente correlacionada con la variable \"velocity_24h\" (0.457) y la variable \"velocity_4w\" (0.384), lo que indica una relación positiva entre la velocidad de transacciones en diferentes períodos de tiempo.\n",
        "\n",
        "- La variable \"credit_risk_score\" muestra una correlación positiva moderada con la variable \"bank_months_count\" (0.501), lo que sugiere que los clientes con mayor antigüedad en el banco tienen una mejor puntuación de riesgo crediticio.\n",
        "\n",
        "- La variable \"proposed_credit_limit\" tiene una correlación positiva moderada con la variable \"credit_risk_score\" (0.661), lo que indica que los clientes con una mejor puntuación de riesgo crediticio tienden a tener límites de crédito más altos propuestos.\n",
        "\n",
        "- La variable \"month\" muestra una correlación negativa moderada con la variable \"velocity_4w\" (-0.837), lo que sugiere una disminución en la velocidad de transacciones a lo largo del tiempo.\n",
        "\n",
        "\n",
        "- La variable \"name_email_similarity\" muestra una correlación positiva moderada con la variable \"credit_risk_score\" (0.060), lo que sugiere que los clientes con una mayor similitud entre su nombre y dirección de correo electrónico tienden a tener una mejor puntuación de riesgo crediticio.\n",
        "\n",
        "- La variable \"income\" tiene una correlación negativa moderada con la variable \"zip_count_4w\" (-0.079), lo que indica que los clientes con ingresos más altos tienden a tener menos códigos postales únicos asociados en un período de 4 semanas.\n",
        "\n",
        "- La variable \"prev_address_months_count\" muestra una correlación positiva moderada con la variable \"zip_count_4w\" (0.128), lo que sugiere que los clientes que han vivido en una dirección por más tiempo tienden a tener más códigos postales únicos asociados en un período de 4 semanas.\n",
        "\n",
        "- La variable \"current_address_months_count\" tiene una correlación positiva moderada con la variable \"zip_count_4w\" (0.198), lo que indica que los clientes que han vivido en su dirección actual por más tiempo tienden a tener más códigos postales únicos asociados en un período de 4 semanas.\n",
        "\n",
        "- La variable \"income\" muestra una correlación positiva moderada con la variable \"session_length_in_minutes\" (0.087), lo que sugiere que los clientes con ingresos más altos tienden a tener sesiones más largas.\n",
        "\n",
        "- La variable \"name_email_similarity\" tiene una correlación positiva moderada con la variable \"session_length_in_minutes\" (0.074), lo que indica que los clientes con una mayor similitud entre su nombre y dirección de correo electrónico tienden a tener sesiones más largas.\n",
        "\n",
        "- La variable \"credit_risk_score\" muestra una correlación positiva moderada con la variable \"intended_balcon_amount\" (0.135), lo que sugiere que los clientes con una mejor puntuación de riesgo crediticio tienden a solicitar montos más altos para préstamos o créditos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "66c034e4",
      "metadata": {
        "cell_id": "00010-41569a5a-d2db-40c6-a236-99a8d5ef58b7",
        "deepnote_cell_type": "markdown"
      },
      "source": [
        "## 4. Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1799cb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Antes recordemos un poco como era el df\n",
        "df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1598e066",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18442de4",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_unique_values = df.nunique()\n",
        "num_unique_values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c94df42b",
      "metadata": {},
      "source": [
        "### 4.1 Declarar `ColumnTransformer`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c4649920",
      "metadata": {},
      "source": [
        "### 4. Preparación de Datos [1 Punto]\n",
        "\n",
        "*Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo.*\n",
        "\n",
        "Dentro de los aspectos minimos que se les solicitarán en este desarrollo estan:\n",
        "\n",
        "- Definir preprocesadores para datos categóricos y ordinales.\n",
        "- Setear las transformaciones en un `ColumnTransformer`.\n",
        "- Transformar todo el datset.\n",
        "\n",
        "**Notas**: \n",
        "\n",
        "- Las variables `id` y `fraud_bool` no deben ser transformadas, pero si pasadas a la siguiente etapa. (Estudiar la transformación `passthrough`).\n",
        "- La salida de la transformación debe ser un pandas dataframe. Para esto, investiguen acerca de la nueva [`set_output` API](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html). Si se levanta un error por matriz sparse, desactiven su uso en la transformación correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e9bac78",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Antes de seguir, clasifiquemos en las siguiente lista las variables ordinales y categoricas:\n",
        "\n",
        "ordinal_features = ['income', 'credit_risk_score']\n",
        "categorical_features = [\n",
        "                        'payment_type', 'employment_status', 'housing_status', 'source', 'device_os',\n",
        "                        'segmentacion_etaria','has_other_cards', 'foreign_request', 'email_is_free',\n",
        "                        'phone_home_valid', 'phone_mobile_valid', 'keep_alive_session'\n",
        "                        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d04d0cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "''' \n",
        "from sklearn.compose import ColumnTransformer y from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
        "se utilizan para definir y aplicar transformaciones específicas a diferentes conjuntos de características en un pipeline \n",
        "de preprocesamiento. ColumnTransformer permite aplicar transformaciones de manera selectiva a columnas específicas, \n",
        "mientras que StandardScaler y OneHotEncoder son preprocesadores comunes utilizados para estandarizar variables numéricas y\n",
        "codificar variables categóricas, respectivamente.\n",
        "\n",
        "'''\n",
        "\n",
        "# Defininiendo preprocesadores para datos categóricos y ordinales \n",
        "\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "ordinal_transformer = StandardScaler()\n",
        "\n",
        "# Seteando las transformaciones en un ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('ord', ordinal_transformer, ordinal_features),\n",
        "        ('passthrough', 'passthrough', ['id', 'fraud_bool'])\n",
        "    ])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1a23ec24",
      "metadata": {},
      "source": [
        "### 4.2 Transformar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749ab504",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "\n",
        "''' \n",
        "from sklearn import set_config nos permite configurar opciones globales en scikit-learn. \n",
        "En este caso, usamos set_config(transform_output=\"pandas\") para obtener la salida de la transformación \n",
        "como un DataFrame de pandas directamente. Esto evita la necesidad de convertir manualmente los datos transformados en un DataFrame.\n",
        "'''\n",
        "\n",
        "# Transformando el dataset df y seteando la salida como un dataframe\n",
        "set_config(transform_output=\"pandas\")\n",
        "preprocessed_data = preprocessor.fit_transform(df)\n",
        "\n",
        "# Convertiendo el resultado en un DataFrame\n",
        "preprocessed_df = pd.DataFrame(preprocessed_data)\n",
        "\n",
        "#Llamando a la bd final db, ya que usamos mucho df \n",
        "db = preprocessed_df\n",
        "db.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "40b95e20-857d-4e29-8071-0071dfd3f62f",
      "metadata": {},
      "source": [
        "## 5. Visualización en Baja Dimensionalidad"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8f55ee74",
      "metadata": {},
      "source": [
        "### 5.1 Muestrear dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e169dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a699143c",
      "metadata": {},
      "source": [
        "### 5.2 Proyectar y agregar proyecciones al dataframe de muestreo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbe5843",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "491bdaf7",
      "metadata": {},
      "source": [
        "### 5.3 Visualizar según rangos etarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94ac394",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ca2ac9bc-c472-49d6-81fc-ccfd2a07b92e",
      "metadata": {},
      "source": [
        "## 6. Anomalías"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "723aa232",
      "metadata": {},
      "source": [
        "### 6.1 Implementar detector de anomalías sobre dataframe de muestreo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bf5300b",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e9a26f75",
      "metadata": {},
      "source": [
        "### 6.2 Agregar resultados a dataframe de muestreo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae3a3402",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "31faab8b",
      "metadata": {},
      "source": [
        "### 6.3 Visualizar según rangos etarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f67978",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "61449e87",
      "metadata": {},
      "source": [
        "### 6.4 Calcular ratios y responder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd07169f",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    }
  ],
  "metadata": {
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "cd4ffb8b-90a0-4648-9d80-2b8c0eef5325",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
